{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792e4160",
   "metadata": {},
   "source": [
    "Dayton Berezoski 228005087"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27708d89",
   "metadata": {},
   "source": [
    "Note: I am not allowed to distribute the data from the MIMIC-IV and eICU databases, so they are not included in the submission. However, the respective SQL queries to get the exact same csv files will be included with this submission if you would like to run the code yourself and have access to these databases. As such the outputs for processing the data will be removed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972704e0",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6ca9aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "06eb93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "502e755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores (pred, actual):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(actual, pred, pos_label = 1)\n",
    "    aucroc = metrics.auc(fpr,tpr)\n",
    "    exp_deaths = np.count_nonzero(pred == 1)\n",
    "    act_deaths = actual.value_counts()[1]\n",
    "    smr = act_deaths/exp_deaths\n",
    "\n",
    "    print('AUC-ROC:', aucroc)\n",
    "    print('SMR:', smr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0522de0",
   "metadata": {},
   "source": [
    "MIMIC-IV (train) data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ec13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m = pd.read_csv('MIMIC_IV.csv')\n",
    "data_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m['death'] = data_m['death'].notnull().astype('int')\n",
    "\n",
    "data_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_m = data_m['death']\n",
    "Y_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fde39417",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m = data_m.drop(['subject_id', 'hadm_id', 'death'], axis=1)\n",
    "categorical_m = x_m[['gender', 'eye_resp_f', 'verbal_resp_f', 'motor_resp_f', 'eye_resp_l', 'verbal_resp_l', 'motor_resp_l']]\n",
    "numeric_m = x_m.drop(['gender', 'eye_resp_f', 'verbal_resp_f', 'motor_resp_f', 'eye_resp_l', 'verbal_resp_l', 'motor_resp_l'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f464256",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_m = pd.get_dummies(categorical_m, columns=['gender'], drop_first=True)\n",
    "categorical_m = pd.get_dummies(categorical_m, columns=['eye_resp_f'], drop_first=True)\n",
    "categorical_m = pd.get_dummies(categorical_m, columns=['verbal_resp_f'], drop_first=True)\n",
    "categorical_m = pd.get_dummies(categorical_m, columns=['motor_resp_f'], drop_first=True)\n",
    "categorical_m = pd.get_dummies(categorical_m, columns=['eye_resp_l'], drop_first=True)\n",
    "categorical_m = pd.get_dummies(categorical_m, columns=['verbal_resp_l'], drop_first=True)\n",
    "categorical_m = pd.get_dummies(categorical_m, columns=['motor_resp_l'], drop_first=True)\n",
    "\n",
    "categorical_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f7e68",
   "metadata": {},
   "source": [
    "eICU (test) data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_e = pd.read_csv('eICU.csv')\n",
    "data_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_norm = {'Male' : 'M', 'Female' : 'F', 'Unknown' : float('NaN')}\n",
    "death_norm = {'Death' : 1, 'Home' : 0, 'Rehabilitation' : 0, 'Skilled Nursing Facility' : 0, 'Other' : 0, 'Other External' : 0, 'Other Hospital' : 0, 'Nursing Home' : 0, '' : 0}\n",
    "\n",
    "data_e['gender'].replace(gen_norm, inplace=True)\n",
    "data_e['death'].replace(death_norm, inplace=True)\n",
    "data_e['tem_f'] = (data_e['tem_f'] * (9/5)) + 32\n",
    "data_e['tem_l'] = (data_e['tem_l'] * (9/5)) + 32\n",
    "data_e['age'] = pd.to_numeric(data_e['age'], errors='coerce')\n",
    "data_e = data_e.dropna().reset_index()\n",
    "data_e = data_e.drop(['index'], axis=1)\n",
    "\n",
    "data_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb506224",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_e = data_e['death'].astype(int)\n",
    "Y_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7e8afc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_e = data_e.drop(['patientunitstayid', 'death'], axis=1)\n",
    "categorical_e = x_e[['gender', 'eye_resp_f', 'verbal_resp_f', 'motor_resp_f', 'eye_resp_l', 'verbal_resp_l', 'motor_resp_l']]\n",
    "numeric_e = x_e.drop(['gender', 'eye_resp_f', 'verbal_resp_f', 'motor_resp_f', 'eye_resp_l', 'verbal_resp_l', 'motor_resp_l'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_e = pd.get_dummies(categorical_e, columns=['gender'], drop_first=True)\n",
    "categorical_e = pd.get_dummies(categorical_e, columns=['eye_resp_f'], drop_first=True)\n",
    "categorical_e = pd.get_dummies(categorical_e, columns=['verbal_resp_f'], drop_first=True)\n",
    "categorical_e = pd.get_dummies(categorical_e, columns=['motor_resp_f'], drop_first=True)\n",
    "categorical_e = pd.get_dummies(categorical_e, columns=['eye_resp_l'], drop_first=True)\n",
    "categorical_e = pd.get_dummies(categorical_e, columns=['verbal_resp_l'], drop_first=True)\n",
    "categorical_e = pd.get_dummies(categorical_e, columns=['motor_resp_l'], drop_first=True)\n",
    "\n",
    "categorical_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19368b6",
   "metadata": {},
   "source": [
    "Combine numeric values to get the normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = len(numeric_m.index)\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ab3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_tot = numeric_m.append(numeric_e, ignore_index=True, sort=False)\n",
    "numeric_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229031e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_tot = (numeric_tot-numeric_tot.mean())/numeric_tot.std()\n",
    "numeric_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242fc4c",
   "metadata": {},
   "source": [
    "Finalizing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6955bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_m = numeric_tot.iloc[:split,:]\n",
    "numeric_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m = pd.concat([categorical_m, numeric_m], axis = 1)\n",
    "X_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee991050",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_e = numeric_tot.iloc[split:,:].reset_index()\n",
    "numeric_e = numeric_e.drop(['index'], axis=1)\n",
    "numeric_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_e = pd.concat([categorical_e, numeric_e], axis = 1)\n",
    "X_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0dca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_e = X_e[X_m.columns]\n",
    "X_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2beb417",
   "metadata": {},
   "source": [
    "Making sure columns are in the correct order. (might need to increase the number of ouputs allowed to see all of the features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c03355d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_M\n",
      "eye_resp_f_2\n",
      "eye_resp_f_3\n",
      "eye_resp_f_4\n",
      "verbal_resp_f_2\n",
      "verbal_resp_f_3\n",
      "verbal_resp_f_4\n",
      "verbal_resp_f_5\n",
      "motor_resp_f_2\n",
      "motor_resp_f_3\n",
      "motor_resp_f_4\n",
      "motor_resp_f_5\n",
      "motor_resp_f_6\n",
      "eye_resp_l_2\n",
      "eye_resp_l_3\n",
      "eye_resp_l_4\n",
      "verbal_resp_l_2\n",
      "verbal_resp_l_3\n",
      "verbal_resp_l_4\n",
      "verbal_resp_l_5\n",
      "motor_resp_l_2\n",
      "motor_resp_l_3\n",
      "motor_resp_l_4\n",
      "motor_resp_l_5\n",
      "motor_resp_l_6\n",
      "age\n",
      "bpm_f\n",
      "bpm_l\n",
      "resp_rate_f\n",
      "tem_f\n",
      "tem_l\n",
      "resp_rate_l\n",
      "O2_sat_f\n",
      "O2_sat_l\n",
      "bps_f\n",
      "bps_l\n",
      "bpd_f\n",
      "bpd_l\n",
      "bpsm_f\n",
      "bpsm_l\n"
     ]
    }
   ],
   "source": [
    "for col in X_m:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d48a226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_M\n",
      "eye_resp_f_2\n",
      "eye_resp_f_3\n",
      "eye_resp_f_4\n",
      "verbal_resp_f_2\n",
      "verbal_resp_f_3\n",
      "verbal_resp_f_4\n",
      "verbal_resp_f_5\n",
      "motor_resp_f_2\n",
      "motor_resp_f_3\n",
      "motor_resp_f_4\n",
      "motor_resp_f_5\n",
      "motor_resp_f_6\n",
      "eye_resp_l_2\n",
      "eye_resp_l_3\n",
      "eye_resp_l_4\n",
      "verbal_resp_l_2\n",
      "verbal_resp_l_3\n",
      "verbal_resp_l_4\n",
      "verbal_resp_l_5\n",
      "motor_resp_l_2\n",
      "motor_resp_l_3\n",
      "motor_resp_l_4\n",
      "motor_resp_l_5\n",
      "motor_resp_l_6\n",
      "age\n",
      "bpm_f\n",
      "bpm_l\n",
      "resp_rate_f\n",
      "tem_f\n",
      "tem_l\n",
      "resp_rate_l\n",
      "O2_sat_f\n",
      "O2_sat_l\n",
      "bps_f\n",
      "bps_l\n",
      "bpd_f\n",
      "bpd_l\n",
      "bpsm_f\n",
      "bpsm_l\n"
     ]
    }
   ],
   "source": [
    "for col in X_e:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df81e5",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f06610f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 1 splits\n",
      "Done with 2 splits\n",
      "Done with 3 splits\n",
      "Done with 4 splits\n",
      "\n",
      "AUC-ROC scores: [0.8743968679490367, 0.8721455696836705, 0.867393385564546, 0.8558037493055971]\n",
      "Mean AUC-ROC score: 0.8674348931257125\n",
      "SMR scores: [1.189463955637708, 1.2199052132701422, 1.231578947368421, 1.2377285851780557]\n",
      "Mean SMR score: 1.2196691753635818\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 4)\n",
    "AUC_ROC_scores = []\n",
    "SMR_scores = []\n",
    "i = 1\n",
    "\n",
    "for train_idx, test_idx in skf.split(X_m,Y_m):\n",
    "    X_train, X_test = X_m.iloc[train_idx,:], X_m.iloc[test_idx,:]\n",
    "    y_train, y_test = Y_m.iloc[train_idx], Y_m.iloc[test_idx]\n",
    "    \n",
    "    logistic_model = LogisticRegression(max_iter = 100000).fit(X_train, y_train, sample_weight=None)\n",
    "\n",
    "    pred = logistic_model.predict(X_test)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, pred)\n",
    "    auroc = metrics.auc(fpr,tpr)\n",
    "\n",
    "    exp_deaths = np.count_nonzero(pred == 1)\n",
    "    act_deaths = y_test.value_counts()[1]\n",
    "    smr = act_deaths/exp_deaths\n",
    "\n",
    "    AUC_ROC_scores.append(auroc)\n",
    "    SMR_scores.append(smr)\n",
    "    \n",
    "    print('Done with', i,'splits')\n",
    "    i += 1\n",
    "\n",
    "print()\n",
    "print('AUC-ROC scores:', AUC_ROC_scores)\n",
    "print('Mean AUC-ROC score:', np.mean(AUC_ROC_scores))\n",
    "print('SMR scores:', SMR_scores)\n",
    "print('Mean SMR score:', np.mean(SMR_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1fcd7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(max_iter = 100000).fit(X_m, Y_m, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dac08043",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_log = logistic_model.predict(X_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "62218c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.6907740874466239\n",
      "SMR: 1.046468401486989\n"
     ]
    }
   ],
   "source": [
    "scores(pred_log, Y_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2108b89",
   "metadata": {},
   "source": [
    "Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7199ff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 3\n",
      "Train AUC-ROC score: 0.9732698862543125\n",
      "Validation AUC-ROC score: 0.9653314174397318\n",
      "\n",
      "Max Depth: 4\n",
      "Train AUC-ROC score: 0.9750060008717129\n",
      "Validation AUC-ROC score: 0.9661369132160071\n",
      "\n",
      "Max Depth: 5\n",
      "Train AUC-ROC score: 0.9784176311956357\n",
      "Validation AUC-ROC score: 0.9663605450096404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(3, 6):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_m, Y_m, test_size=0.2, random_state=315)\n",
    "    GB = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01, max_depth=i, subsample=0.5, n_iter_no_change=10)\n",
    "    GB.fit(X_train, y_train)\n",
    "    y_train_pred = GB.predict_proba(X_train)[:,1]\n",
    "    y_valid_pred = GB.predict_proba(X_valid)[:,1]\n",
    "    print('Max Depth:', i)\n",
    "    print('Train AUC-ROC score:',metrics.roc_auc_score(y_train, y_train_pred))\n",
    "    print('Validation AUC-ROC score:',metrics.roc_auc_score(y_valid, y_valid_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9f355170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.6561           0.0136            1.69m\n",
      "         2           0.6479           0.0122            1.62m\n",
      "         3           0.6239           0.0119            1.72m\n",
      "         4           0.6212           0.0109            1.65m\n",
      "         5           0.6136           0.0102            1.72m\n",
      "         6           0.5973           0.0097            1.80m\n",
      "         7           0.6021           0.0090            1.82m\n",
      "         8           0.5763           0.0086            1.85m\n",
      "         9           0.5676           0.0084            1.81m\n",
      "        10           0.5670           0.0078            1.80m\n",
      "        20           0.4978           0.0054            1.64m\n",
      "        30           0.4497           0.0040            1.58m\n",
      "        40           0.4157           0.0032            1.54m\n",
      "        50           0.3879           0.0025            1.52m\n",
      "        60           0.3623           0.0022            1.50m\n",
      "        70           0.3457           0.0018            1.50m\n",
      "        80           0.3226           0.0016            1.51m\n",
      "        90           0.3090           0.0013            1.49m\n",
      "       100           0.3031           0.0011            1.46m\n",
      "       200           0.2265           0.0003            1.27m\n",
      "       300           0.1952           0.0002            1.18m\n",
      "       400           0.1877           0.0000            1.07m\n",
      "       500           0.1754           0.0000           53.60s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.01, max_depth=5, n_estimators=1000,\n",
       "                           n_iter_no_change=10, subsample=0.5, verbose=1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01, max_depth=i, subsample=0.5, n_iter_no_change=10, verbose=1)\n",
    "GB.fit(X_m, Y_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3d502238",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gb = GB.predict(X_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "02a83007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.7607446526990743\n",
      "SMR: 0.7781617138908086\n"
     ]
    }
   ],
   "source": [
    "scores(pred_gb, Y_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c207f",
   "metadata": {},
   "source": [
    "Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3ea73179",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e603a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(n, val_frac, seed):\n",
    "    # Determine the size of the validation set\n",
    "    n_val = int(val_frac * n)\n",
    "    np.random.seed(seed)\n",
    "    # Create random permutation between 0 to n-1\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Pick first n_val indices for validation set\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1ea5b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = F.relu(self.fc1(X))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cae2f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(40,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "01354d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "26bd127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_m, Y_m, test_size=0.2, random_state=315)\n",
    "\n",
    "X_train_tensor = torch.Tensor(X_train.values.astype(np.float32))\n",
    "y_train_tensor = torch.Tensor(y_train.values.astype(np.float32))\n",
    "y_train_tensor = y_train_tensor.long()\n",
    "train_tensor = data_utils.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "X_valid_tensor = torch.Tensor(X_valid.values.astype(np.float32))\n",
    "y_valid_tensor = torch.Tensor(y_valid.values.astype(np.float32))\n",
    "y_valid_tensor = y_valid_tensor.long()\n",
    "valid_tensor = data_utils.TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "valid_loader = data_utils.DataLoader(dataset = valid_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "854873a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr):\n",
    "\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        val_loss = 0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        num_corr = 0.0\n",
    "        num_tot = 0.0\n",
    "\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    " \n",
    "            X = x.to(device=device)\n",
    "            Y = y.to(device=device)\n",
    "            Y = Y.squeeze()\n",
    "\n",
    "            opt_fn.zero_grad()\n",
    "            hyp = model(X)\n",
    "\n",
    "            cost = loss_fn(hyp, Y)\n",
    "            cost.backward()\n",
    "\n",
    "            opt_fn.step()\n",
    "\n",
    "            pred = hyp.data.max(dim=1)[1]\n",
    "\n",
    "            num_corr += (pred == y).sum()\n",
    "            num_tot += pred.size(0)\n",
    "\n",
    "            train_loss += cost.item()\n",
    "\n",
    "        train_accuracy = 0\n",
    "\n",
    "        if num_tot > 0:\n",
    "            train_accuracy =  num_corr / num_tot  \n",
    "\n",
    "        num_corr = 0.0\n",
    "        num_tot = 0.0\n",
    "\n",
    "        for i, (x, y) in enumerate(val_dl):\n",
    "            X = x.to(device=device)\n",
    "            Y = y.to(device=device)\n",
    "            Y = Y.squeeze()\n",
    "\n",
    "            hyp = model(X)\n",
    "            \n",
    "            cost = loss_fn(hyp, Y)\n",
    "\n",
    "            pred = hyp.data.max(dim=1)[1]\n",
    "\n",
    "            num_corr += (pred == y).sum()\n",
    "            num_tot += pred.size(0)\n",
    "\n",
    "            val_loss += cost.item()\n",
    "\n",
    "        val_accuracy = 0\n",
    "\n",
    "        if num_tot > 0:\n",
    "            val_accuracy =  num_corr / num_tot  \n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_accuracy != 0:\n",
    "            print(\"Epoch {}/{}, train_loss: {:.4f}, val_loss: {:.4f}, train_accuracy: {:.4f}, val_accuracy: {:.4f}\"\n",
    "                .format(epoch+1, n_epochs, train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "        else:\n",
    "            print(\"Epoch {}/{}, train_loss: {:.4f}, train_accuracy: {:.4f}\"\n",
    "                .format(epoch+1, n_epochs, train_loss, train_accuracy))\n",
    "\n",
    "    tl = np.array(train_losses).flatten()\n",
    "    ta = np.array(train_accuracies).flatten()\n",
    "    \n",
    "    train_losses = tl\n",
    "    train_accuracies = ta\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5ad99786",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "opt_fn = torch.optim.Adam(network.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f48c6128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, train_loss: 222.1029, val_loss: 45.3543, train_accuracy: 0.8908, val_accuracy: 0.8977\n",
      "Epoch 2/20, train_loss: 139.7828, val_loss: 32.0710, train_accuracy: 0.9599, val_accuracy: 0.9698\n",
      "Epoch 3/20, train_loss: 126.1692, val_loss: 31.3163, train_accuracy: 0.9709, val_accuracy: 0.9697\n",
      "Epoch 4/20, train_loss: 124.5815, val_loss: 31.3818, train_accuracy: 0.9713, val_accuracy: 0.9708\n",
      "Epoch 5/20, train_loss: 122.5610, val_loss: 32.2256, train_accuracy: 0.9713, val_accuracy: 0.9673\n",
      "Epoch 6/20, train_loss: 121.2661, val_loss: 31.3177, train_accuracy: 0.9719, val_accuracy: 0.9700\n",
      "Epoch 7/20, train_loss: 119.7145, val_loss: 30.6741, train_accuracy: 0.9720, val_accuracy: 0.9711\n",
      "Epoch 8/20, train_loss: 118.6946, val_loss: 30.8748, train_accuracy: 0.9721, val_accuracy: 0.9697\n",
      "Epoch 9/20, train_loss: 118.0018, val_loss: 30.5011, train_accuracy: 0.9720, val_accuracy: 0.9702\n",
      "Epoch 10/20, train_loss: 116.1812, val_loss: 30.1433, train_accuracy: 0.9724, val_accuracy: 0.9704\n",
      "Epoch 11/20, train_loss: 116.6403, val_loss: 30.6691, train_accuracy: 0.9723, val_accuracy: 0.9703\n",
      "Epoch 12/20, train_loss: 115.1692, val_loss: 30.6170, train_accuracy: 0.9725, val_accuracy: 0.9697\n",
      "Epoch 13/20, train_loss: 113.6923, val_loss: 31.4867, train_accuracy: 0.9725, val_accuracy: 0.9689\n",
      "Epoch 14/20, train_loss: 113.5455, val_loss: 30.4698, train_accuracy: 0.9730, val_accuracy: 0.9705\n",
      "Epoch 15/20, train_loss: 112.4589, val_loss: 30.4880, train_accuracy: 0.9730, val_accuracy: 0.9703\n",
      "Epoch 16/20, train_loss: 112.7262, val_loss: 30.4051, train_accuracy: 0.9733, val_accuracy: 0.9704\n",
      "Epoch 17/20, train_loss: 111.4323, val_loss: 30.5678, train_accuracy: 0.9733, val_accuracy: 0.9702\n",
      "Epoch 18/20, train_loss: 111.8636, val_loss: 30.6837, train_accuracy: 0.9732, val_accuracy: 0.9707\n",
      "Epoch 19/20, train_loss: 111.2100, val_loss: 30.8738, train_accuracy: 0.9735, val_accuracy: 0.9695\n",
      "Epoch 20/20, train_loss: 109.8512, val_loss: 30.7453, train_accuracy: 0.9737, val_accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "history = train_model(num_epochs, network, train_loader, valid_loader, loss_fn, opt_fn, lr)\n",
    "model, train_losses, val_losses, train_accuracies, val_accuracies = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "96407226",
   "metadata": {},
   "outputs": [],
   "source": [
    "tens = []\n",
    "pred = []\n",
    "batch = []\n",
    "actual = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6c2864c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(valid_loader):\n",
    "    X = x.to(device=device)\n",
    "    Y = y.to(device=device)\n",
    "    Y = Y.squeeze()\n",
    "    hyp = network(X)\n",
    "    tens.append(hyp.data.max(dim=1)[1])\n",
    "    batch.append(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "85d983b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in tens:\n",
    "    inter = t.numpy()\n",
    "    pred = np.append(pred, inter)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4468a2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "9896    0.0\n",
       "9897    0.0\n",
       "9898    1.0\n",
       "9899    0.0\n",
       "9900    0.0\n",
       "Length: 9901, dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in batch:\n",
    "    inter = t.numpy()\n",
    "    actual = np.append(actual, inter)\n",
    "\n",
    "actual = pd.Series(actual)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "76dc2428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.8958510037929814\n",
      "SMR: 1.1092896174863387\n"
     ]
    }
   ],
   "source": [
    "scores(pred,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c5301933",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_x_tensor = torch.Tensor(X_m.values.astype(np.float32))\n",
    "mimic_y_tensor = torch.Tensor(Y_m.values.astype(np.float32))\n",
    "mimic_y_tensor = mimic_y_tensor.long()\n",
    "mimic_tensor = data_utils.TensorDataset(mimic_x_tensor, mimic_y_tensor)\n",
    "mimic_loader = data_utils.DataLoader(dataset = mimic_tensor, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "eicu_x_tensor = torch.Tensor(X_e.values.astype(np.float32))\n",
    "eicu_y_tensor = torch.Tensor(Y_e.values.astype(np.float32))\n",
    "eicu_y_tensor = eicu_y_tensor.long()\n",
    "eicu_tensor = data_utils.TensorDataset(eicu_x_tensor, eicu_y_tensor)\n",
    "eicu_loader = data_utils.DataLoader(dataset = eicu_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "02e852f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, train_loss: 140.1740, val_loss: 82.6697, train_accuracy: 0.9731, val_accuracy: 0.8359\n",
      "Epoch 2/20, train_loss: 140.2334, val_loss: 85.4183, train_accuracy: 0.9732, val_accuracy: 0.8351\n",
      "Epoch 3/20, train_loss: 138.4982, val_loss: 87.9455, train_accuracy: 0.9733, val_accuracy: 0.8396\n",
      "Epoch 4/20, train_loss: 138.3496, val_loss: 88.9449, train_accuracy: 0.9732, val_accuracy: 0.8284\n",
      "Epoch 5/20, train_loss: 137.8287, val_loss: 90.6401, train_accuracy: 0.9736, val_accuracy: 0.8251\n",
      "Epoch 6/20, train_loss: 139.9392, val_loss: 86.9536, train_accuracy: 0.9735, val_accuracy: 0.8295\n",
      "Epoch 7/20, train_loss: 139.1460, val_loss: 84.4417, train_accuracy: 0.9742, val_accuracy: 0.8393\n",
      "Epoch 8/20, train_loss: 136.1653, val_loss: 87.8173, train_accuracy: 0.9737, val_accuracy: 0.8347\n",
      "Epoch 9/20, train_loss: 136.1471, val_loss: 86.7845, train_accuracy: 0.9738, val_accuracy: 0.8353\n",
      "Epoch 10/20, train_loss: 136.0586, val_loss: 86.2785, train_accuracy: 0.9737, val_accuracy: 0.8374\n",
      "Epoch 11/20, train_loss: 134.5564, val_loss: 89.9988, train_accuracy: 0.9740, val_accuracy: 0.8318\n",
      "Epoch 12/20, train_loss: 134.5565, val_loss: 91.3520, train_accuracy: 0.9736, val_accuracy: 0.8280\n",
      "Epoch 13/20, train_loss: 136.9728, val_loss: 89.0944, train_accuracy: 0.9739, val_accuracy: 0.8339\n",
      "Epoch 14/20, train_loss: 133.5655, val_loss: 90.1499, train_accuracy: 0.9741, val_accuracy: 0.8287\n",
      "Epoch 15/20, train_loss: 133.4100, val_loss: 89.3352, train_accuracy: 0.9744, val_accuracy: 0.8262\n",
      "Epoch 16/20, train_loss: 133.1962, val_loss: 90.0508, train_accuracy: 0.9743, val_accuracy: 0.8331\n",
      "Epoch 17/20, train_loss: 133.0842, val_loss: 88.5885, train_accuracy: 0.9744, val_accuracy: 0.8342\n",
      "Epoch 18/20, train_loss: 132.6618, val_loss: 87.3698, train_accuracy: 0.9744, val_accuracy: 0.8324\n",
      "Epoch 19/20, train_loss: 132.4361, val_loss: 90.5782, train_accuracy: 0.9744, val_accuracy: 0.8364\n",
      "Epoch 20/20, train_loss: 131.9148, val_loss: 89.8768, train_accuracy: 0.9748, val_accuracy: 0.8289\n"
     ]
    }
   ],
   "source": [
    "history = train_model(num_epochs, network, mimic_loader, eicu_loader, loss_fn, opt_fn, lr)\n",
    "model, train_losses, val_losses, train_accuracies, val_accuracies = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "64e61aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tens = []\n",
    "pred = []\n",
    "batch = []\n",
    "actual = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a83eabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(eicu_loader):\n",
    "    X = x.to(device=device)\n",
    "    Y = y.to(device=device)\n",
    "    Y = Y.squeeze()\n",
    "    hyp = network(X)\n",
    "    tens.append(hyp.data.max(dim=1)[1])\n",
    "    batch.append(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "352b0c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in tens:\n",
    "    inter = t.numpy()\n",
    "    pred = np.append(pred, inter)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3577936c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       1.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "6249    1.0\n",
       "6250    0.0\n",
       "6251    0.0\n",
       "6252    1.0\n",
       "6253    0.0\n",
       "Length: 6254, dtype: float64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in batch:\n",
    "    inter = t.numpy()\n",
    "    actual = np.append(actual, inter)\n",
    "\n",
    "actual = pd.Series(actual)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0151b72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.7352201059623201\n",
      "SMR: 0.8866141732283465\n"
     ]
    }
   ],
   "source": [
    "scores(pred,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a795d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
